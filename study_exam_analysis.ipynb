{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 1:**\n",
    "\n",
    "### **a. Airline Flight Schedules**\n",
    "- **Availability:** **Publicly available (partially restricted in some cases)**\n",
    "- **Justification:**\n",
    "  - Many airlines provide **real-time flight schedules** through **their websites and APIs** such as:\n",
    "    - [FlightAware API](https://flightaware.com/commercial/flightxml/) (Real-time flight tracking)\n",
    "    - [OpenSky Network](https://opensky-network.org/) (Open-source flight tracking)\n",
    "    - [FAA Flight Data](https://www.faa.gov/data_research/) (Federal Aviation Administration)\n",
    "  - **Publicly available data includes:**\n",
    "    - Departure and arrival times\n",
    "    - Flight numbers and airlines\n",
    "    - Gate assignments and estimated delays\n",
    "  - **Restricted data includes:**\n",
    "    - Passenger manifests and bookings (restricted for privacy)\n",
    "    - Real-time air traffic control communication (restricted for security)\n",
    "    - Operational logistics (available only to airline staff or premium users)\n",
    "\n",
    "---\n",
    "\n",
    "### **b. University Admission Statistics**\n",
    "- **Availability:** **Partially public, mostly restricted**\n",
    "- **Justification:**\n",
    "  - General admission statistics (e.g., acceptance rates, SAT scores, student demographics) are **publicly available** on:\n",
    "    - [Common Data Set Initiative](https://commondataset.org/) (Detailed admission reports)\n",
    "    - [National Center for Education Statistics (NCES)](https://nces.ed.gov/) (Federal education data)\n",
    "    - Individual university websites often publish their own reports\n",
    "  - **Restricted data includes:**\n",
    "    - Individual applicant details (protected under [FERPA](https://www2.ed.gov/policy/gen/guid/fpco/ferpa/index.html))\n",
    "    - Admission committee notes, essays, and recommendations (confidential)\n",
    "    - Financial aid decisions (restricted to the applicant)\n",
    "\n",
    "---\n",
    "\n",
    "### **c. Crime Statistics in Your City**\n",
    "- **Availability:** **Publicly available (some details restricted for privacy and security)**\n",
    "- **Justification:**\n",
    "  - Many law enforcement agencies provide open access to crime data for public awareness and research. Some official sources include:\n",
    "    - [FBI Crime Data Explorer](https://cde.ucr.cjis.gov/) (Nationwide crime reports)\n",
    "    - [NYPD Crime Map](https://maps.nyc.gov/crime/) (Crime data for New York City)\n",
    "    - [Bureau of Justice Statistics (BJS)](https://bjs.ojp.gov/) (Criminal justice system data)\n",
    "  - **Publicly available data includes:**\n",
    "    - Number and type of crimes reported\n",
    "    - Crime rates per neighborhood or district\n",
    "    - Trends in violent vs. non-violent crimes\n",
    "  - **Restricted data includes:**\n",
    "    - Personal information of victims and suspects (protected by privacy laws)\n",
    "    - Ongoing investigations (restricted to law enforcement agencies)\n",
    "    - Juvenile crime records (protected under privacy regulations)\n",
    "\n",
    "---\n",
    "\n",
    "### **d. Movie Box Office Revenue**\n",
    "- **Availability:** **Publicly available (some financial data restricted)**\n",
    "- **Justification:**\n",
    "  - Box office revenue data is widely available through industry sources:\n",
    "    - [Box Office Mojo](https://www.boxofficemojo.com/) (Detailed box office earnings)\n",
    "    - [The Numbers](https://www.the-numbers.com/) (Box office and financial insights)\n",
    "    - [IMDB Box Office Data](https://www.imdb.com/chart/boxoffice/) (Basic box office reports)\n",
    "  - **Publicly available data includes:**\n",
    "    - Daily and weekend box office earnings\n",
    "    - Gross revenue by region (domestic/international)\n",
    "    - Movie rankings based on earnings\n",
    "  - **Restricted data includes:**\n",
    "    - Profit and loss breakdowns (hidden by studios for competitive reasons)\n",
    "    - Revenue from streaming and licensing deals (not publicly disclosed)\n",
    "    - Marketing and distribution costs (often estimates, not official figures)\n",
    "\n",
    "---\n",
    "\n",
    "### **e. Daily Weather Temperature in Your City**\n",
    "- **Availability:** **Publicly available (some proprietary models restricted)**\n",
    "- **Justification:**\n",
    "  - Most weather data is **freely available** through government agencies and open APIs:\n",
    "    - [National Weather Service (NWS)](https://www.weather.gov/) (Official US weather data)\n",
    "    - [NOAA Climate Data](https://www.ncdc.noaa.gov/) (Historical climate and temperature records)\n",
    "    - [OpenWeatherMap API](https://openweathermap.org/) (Real-time weather API)\n",
    "    - [The Weather Channel](https://weather.com/) (Global weather reports)\n",
    "  - **Publicly available data includes:**\n",
    "    - Current temperature, humidity, and wind speed\n",
    "    - Severe weather warnings (hurricanes, tornadoes, storms)\n",
    "    - Historical weather trends for research purposes\n",
    "  - **Restricted data includes:**\n",
    "    - Proprietary weather forecasting models (owned by private companies like AccuWeather)\n",
    "    - Military or classified weather data (used for defense operations)\n",
    "    - Specialized climate analysis that requires paid access\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "| **Data Source**                     | **Availability**               | **References** |\n",
    "|-------------------------------------|-------------------------------|---------------|\n",
    "| **Airline Flight Schedules**        | Public (basic data), Restricted (detailed) | [FlightAware](https://flightaware.com/commercial/flightxml/), [OpenSky](https://opensky-network.org/) |\n",
    "| **University Admission Statistics** | Public (general data), Restricted (personal data) | [Common Data Set](https://commondataset.org/), [NCES](https://nces.ed.gov/) |\n",
    "| **Crime Statistics**                | Public (general data), Restricted (sensitive data) | [FBI Crime Explorer](https://cde.ucr.cjis.gov/), [NYPD Crime Map](https://maps.nyc.gov/crime/) |\n",
    "| **Movie Box Office Revenue**        | Public (gross revenue), Restricted (detailed finances) | [Box Office Mojo](https://www.boxofficemojo.com/), [The Numbers](https://www.the-numbers.com/) |\n",
    "| **Daily Weather Data**              | Public (forecasts), Restricted (proprietary models) | [NWS](https://www.weather.gov/), [NOAA](https://www.ncdc.noaa.gov/) |"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## **Question 2:**\n",
    "\n",
    "## **1. UFC Fight Statistics Dataset**\n",
    "**Description:**\n",
    "This dataset contains detailed statistics for UFC fights, including information on fighters, match results, strikes landed, takedown success rates, and fight duration. It can be used to analyze trends in fighter performance, strategy effectiveness, and fight outcomes.\n",
    "\n",
    "- **Source:** [UFC Stats](http://statleaders.ufc.com/)\n",
    "- **Alternative:** [Ultimate UFC Dataset on Kaggle](https://www.kaggle.com/datasets/mdabbert/ultimate-ufc-dataset)\n",
    "- **GitHub Scraper:** [UFC Stats Scraper](https://github.com/Greco1899/scrape_ufc_stats)\n",
    "\n",
    "**Potential Analyses:**\n",
    "1. **Fighter Performance Over Time**\n",
    "   - Track how a fighter's striking accuracy, takedown success, and defense evolve over their career.\n",
    "   - Identify peak performance years and possible decline phases.\n",
    "\n",
    "2. **Winning Strategies by Weight Class**\n",
    "   - Compare statistics between different weight classes to identify the most common winning strategies (e.g., striking-heavy vs. grappling-heavy).\n",
    "   - Determine if certain attributes (height, reach, fight style) correlate with higher win rates.\n",
    "\n",
    "3. **Predicting Fight Outcomes**\n",
    "   - Use machine learning models to predict the winner of an upcoming fight based on historical data.\n",
    "   - Factors like age, previous fight history, reach advantage, and striking accuracy can be analyzed to improve predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Chess Game Data**\n",
    "**Description:**\n",
    "This dataset contains millions of chess games played at various levels, from casual online games to world championship matches. It includes details like player ratings, moves played, opening strategies, and game outcomes.\n",
    "\n",
    "- **Source:** [Lichess Game Database](https://database.lichess.org/)\n",
    "- **Alternative:** [FICS Chess Database](https://www.ficsgames.org/download.html)\n",
    "- **PGN Database:** [Chess.com PGN Downloads](https://www.chess.com/games/archive)\n",
    "\n",
    "**Potential Analyses:**\n",
    "1. **Opening Strategy Effectiveness**\n",
    "   - Analyze win rates for various chess openings (e.g., Sicilian Defense vs. King's Indian).\n",
    "   - Identify which openings lead to the highest success rates for White and Black at different rating levels.\n",
    "\n",
    "2. **Player Rating Progression**\n",
    "   - Track how a player's Elo rating changes over time and identify factors contributing to rapid improvement or stagnation.\n",
    "   - Compare rating progressions of titled players (e.g., Grandmasters) vs. casual players.\n",
    "\n",
    "3. **AI vs. Human Playstyles**\n",
    "   - Compare human gameplay trends with AI engines like Stockfish.\n",
    "   - Determine how AI-influenced strategies (e.g., AlphaZero-inspired moves) have changed human play over time.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. NASA Exoplanet Data**\n",
    "**Description:**\n",
    "This dataset provides detailed information on exoplanets discovered beyond our solar system, including planet size, orbital period, distance from their host star, and potential habitability factors.\n",
    "\n",
    "- **Source:** [NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/)\n",
    "- **Alternative:** [Exoplanet Data from Kaggle](https://www.kaggle.com/datasets/nasa/kepler-exoplanet-search-results)\n",
    "- **JWST:** [https://webbtelescope.org/contents/articles/webbs-impact-on-exoplanet-research]\n",
    "\n",
    "**Potential Analyses:**\n",
    "1. **Habitability Score Prediction**\n",
    "   - Use exoplanet parameters (e.g., temperature, atmospheric conditions, distance from the star) to assess the likelihood of supporting life.\n",
    "   - Identify the most Earth-like exoplanets discovered.\n",
    "\n",
    "2. **Exoplanet Size Distribution**\n",
    "   - Analyze the frequency of different exoplanet sizes (e.g., Earth-sized, Jupiter-sized).\n",
    "   - Determine whether certain types of stars are more likely to host larger or smaller planets.\n",
    "\n",
    "3. **Orbital Patterns and Star Types**\n",
    "   - Study the relationship between a planetâ€™s orbital characteristics (e.g., eccentricity, distance) and its host starâ€™s properties (e.g., mass, temperature).\n",
    "   - Identify trends that could guide future exoplanet searches."
   ],
   "id": "75d9a0eb52a54dc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 3:**\n",
    "\n",
    "### **Objective**\n",
    "The goal of this experiment is to determine whether my friends prefer the taste of **Regular Coke** or **Diet Coke** through a controlled blind taste test.\n",
    "\n",
    "### **Study Design**\n",
    "1. **Participants:**\n",
    "   - A sample of **at least 20 participants** from my friend group.\n",
    "   - Participants should have **no dietary restrictions** that would prevent them from consuming soda.\n",
    "\n",
    "2. **Experimental Setup:**\n",
    "   - Each participant will be given two cups labeled **A** and **B**.\n",
    "   - One cup will contain **Regular Coke**, and the other will contain **Diet Coke**.\n",
    "   - The order of the drinks will be **randomized** to prevent bias.\n",
    "\n",
    "3. **Blind Taste Test:**\n",
    "   - Participants will **not** be informed which cup contains which soda.\n",
    "   - They will take a sip from each cup and **choose which one they prefer**.\n",
    "   - They will also be asked to **guess which one is Regular Coke** and which one is Diet Coke.\n",
    "\n",
    "4. **Data Collection:**\n",
    "   - **Preferred Drink:** Record whether each participant preferred the drink in **Cup A or Cup B**.\n",
    "   - **Correct Identification:** Track how many participants correctly identified which drink was Regular Coke and which was Diet Coke.\n",
    "   - **Additional Notes:** Participants can provide comments on taste differences.\n",
    "\n",
    "5. **Analysis:**\n",
    "   - **Count the number of votes** for Regular Coke vs. Diet Coke.\n",
    "   - **Perform a statistical test** (e.g., **chi-square test**) to determine if there is a significant preference for one over the other.\n",
    "   - **Analyze identification accuracy** to see if people can correctly distinguish between the two.\n",
    "\n",
    "6. **Conclusion:**\n",
    "   - If significantly more participants choose one drink over the other, we conclude that **one is generally preferred**.\n",
    "   - If the results are close to 50-50, it may suggest that **there is no strong preference**.\n",
    "   - If most participants struggle to correctly identify the drinks, it could indicate that **the taste difference is not obvious**.\n",
    "\n",
    "### **Considerations for Fairness**\n",
    "- **Randomization:** To eliminate order bias, half of the participants receive Regular Coke in Cup A, and the other half receive Diet Coke in Cup A.\n",
    "- **Controlled Conditions:** Ensure that both drinks are **served at the same temperature** and in identical cups to avoid visual bias.\n",
    "- **Avoid Brand Influence:** Participants should not see the cans/bottles to prevent preconceived preferences.\n"
   ],
   "id": "8ad090682be74f9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 4: The Role of Logarithms in Data Analysis**\n",
    "\n",
    "Logarithms are essential mathematical tools used in data analysis for **scaling, transformation, and interpretation** of numerical data. Below are three major use cases:\n",
    "\n",
    "### **1. Data Transformation and Normalization**\n",
    "   - Many real-world datasets exhibit **skewed distributions** (e.g., income levels, population sizes, stock prices).\n",
    "   - Applying a **log transformation** converts highly skewed data into a more **normally distributed** form, making statistical analysis more reliable.\n",
    "   - **Example:** Converting exponential data (e.g., **COVID-19 case growth**) into a linear scale for easier interpretation.\n",
    "\n",
    "### **2. Handling Multiplicative Relationships**\n",
    "   - Logarithms allow **multiplicative relationships** to be converted into **additive** ones.\n",
    "   - This is particularly useful in **machine learning and regression models**, where relationships between variables are often **not linear**.\n",
    "   - **Example:** In economics, the **Cobb-Douglas production function** models output as a product of labor and capital, which can be transformed using logs to estimate the contributions of each factor.\n",
    "\n",
    "### **3. Measuring Growth Rates and Percent Changes**\n",
    "   - Logarithmic scales are commonly used to measure **relative changes** and **growth rates**.\n",
    "   - **Example:** The **logarithmic return** in finance measures the percentage change in stock prices:\n",
    "\n",
    "     $$\n",
    "     r = \\ln \\left(\\frac{P_t}{P_{t-1}}\\right)\n",
    "     $$\n",
    "\n",
    "   where $P_t$ and $P_{t-1}$ are the stock prices at different times.\n",
    "\n",
    "   - Log scales are also used in the **Richter scale (earthquakes)** and **decibels (sound intensity)** to handle large variations in data.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "Logarithms play a vital role in **data science, statistics, and real-world modeling** by simplifying complex relationships, improving data interpretation, and making exponential trends more understandable."
   ],
   "id": "5a08e1ce219eae2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 5:*\n",
    "\n",
    "Correlation and causation are two fundamental concepts in statistics and data analysis. **Correlation** refers to a statistical relationship between two variables, meaning that when one variable changes, the other tends to change as well. However, correlation does **not** imply that one variable directly causes the other to change. **Causation**, on the other hand, indicates a **cause-and-effect relationship**, where changes in one variable **directly lead** to changes in another.\n",
    "\n",
    "A classic example of correlation without causation is the relationship between **ice cream sales and drowning incidents**. Data may show that as ice cream sales increase, drowning incidents also rise. However, this does not mean that eating ice cream causes drowning. Instead, a third factorâ€”**hot weather**â€”is influencing both variables. The heat increases ice cream consumption while also encouraging more people to swim, leading to a higher risk of drowning. This is an example of a **spurious correlation**, where a hidden factor affects both observed variables.\n",
    "\n",
    "To establish causation, **controlled experiments** or advanced statistical techniques such as **randomized controlled trials (RCTs)** and **longitudinal studies** are required. Observing a correlation alone is **not sufficient**â€”researchers must eliminate **confounding variables** and demonstrate a **direct cause-and-effect link**. Without proper experimental design and rigorous statistical analysis, assuming causation from correlation can lead to misleading conclusions.\n",
    "\n",
    "In summary, while correlation can help identify relationships between variables, it does not prove causation. Establishing a true causal link requires careful study, eliminating external factors, and applying proper experimental methods.\n"
   ],
   "id": "68dfa017e9d15e08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 6: Statistical Analysis of Study Hours and Exam Scores**\n",
    "\n",
    "Given the dataset of **hours studied per week (X)** and **exam scores (Y)**, we perform statistical calculations to analyze the relationship between these variables.\n",
    "\n",
    "### **a. Mean of \\( X \\) and \\( Y \\)**\n",
    "The mean represents the **average value** of the dataset.\n",
    "\n",
    "- **Mean of \\( X \\) (Hours of Study per Week):** **8.125**\n",
    "- **Mean of \\( Y \\) (Exam Score):** **73.9**\n",
    "\n",
    "### **b. Standard Deviations of \\( X \\) and \\( Y \\)**\n",
    "The standard deviation measures the **spread** of the data.\n",
    "\n",
    "- **Standard Deviation of \\( X \\):** **3.57**\n",
    "- **Standard Deviation of \\( Y \\):** **13.33**\n",
    "\n",
    "### **c. Covariance between \\( X \\) and \\( Y \\)**\n",
    "Covariance measures how two variables **vary together**.\n",
    "\n",
    "- **Covariance:** **47.30**\n",
    "\n",
    "A **positive covariance** indicates that as **study hours increase, exam scores tend to increase as well**.\n",
    "\n",
    "### **d. Pearson Correlation Coefficient (\\( r \\))**\n",
    "Pearson's correlation coefficient measures the **strength and direction** of a linear relationship.\n",
    "\n",
    "- **Pearson Correlation Coefficient (\\( r \\)) = 0.994**\n",
    "- **Interpretation:**\n",
    "  - The value is **very close to 1**, indicating an **extremely strong positive linear relationship**.\n",
    "  - This means that **as study hours increase, exam scores tend to increase in a highly predictable manner**.\n"
   ],
   "id": "af193b6dae204ac1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:40:32.575544Z",
     "start_time": "2025-02-28T16:40:32.459323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data: Hours of Study (X) and Exam Scores (Y)\n",
    "X = np.array([5, 8, 7, 9, 11, 4.5, 10, 3, 12, 6, 7, 10, 2, 13, 8, 14, 5.5, 3.5, 12.5, 11.5])\n",
    "Y = np.array([62, 74, 69, 76, 85, 60, 80, 55, 90, 65, 70, 82, 50, 92, 78, 95, 63, 58, 88, 86])\n",
    "\n",
    "\n",
    "# Function to compute statistics\n",
    "def compute_statistics(X, Y, dataset_name=\"Dataset\"):\n",
    "    mean_X = np.mean(X)\n",
    "    mean_Y = np.mean(Y)\n",
    "    std_X = np.std(X, ddof=1)  # Sample standard deviation\n",
    "    std_Y = np.std(Y, ddof=1)  # Sample standard deviation\n",
    "    cov_XY = np.cov(X, Y, ddof=1)[0, 1]\n",
    "    pearson_r = np.corrcoef(X, Y)[0, 1]\n",
    "    correlation_strength = \"strong\" if abs(pearson_r) > 0.7 else \"moderate\" if abs(pearson_r) > 0.4 else \"weak\"\n",
    "    correlation_direction = \"positive\" if pearson_r > 0 else \"negative\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{dataset_name:^60}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Mean of X: {mean_X:.2f}\")\n",
    "    print(f\"Mean of Y: {mean_Y:.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Standard Deviation of X: {std_X:.2f}\")\n",
    "    print(f\"Standard Deviation of Y: {std_Y:.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Covariance between X and Y: {cov_XY:.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Pearson Correlation Coefficient (r): {pearson_r:.3f}\")\n",
    "    print(f\"Correlation Strength: {correlation_strength.capitalize()}\")\n",
    "    print(f\"Correlation Direction: {correlation_direction.capitalize()}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Run the function for Study Hours vs Exam Scores\n",
    "compute_statistics(X, Y, dataset_name=\"Study Hours vs Exam Scores\")"
   ],
   "id": "a5f3b254820c1478",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "                 Study Hours vs Exam Scores                 \n",
      "============================================================\n",
      "Mean of X: 8.12\n",
      "Mean of Y: 73.90\n",
      "------------------------------------------------------------\n",
      "Standard Deviation of X: 3.57\n",
      "Standard Deviation of Y: 13.33\n",
      "------------------------------------------------------------\n",
      "Covariance between X and Y: 47.30\n",
      "------------------------------------------------------------\n",
      "Pearson Correlation Coefficient (r): 0.994\n",
      "Correlation Strength: Strong\n",
      "Correlation Direction: Positive\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 7: Spearman Rank Correlation Analysis**\n",
    "\n",
    "Given the dataset of **study hours (X) and exam scores (Y)**, we calculate the **Spearman Rank Correlation Coefficient (Ï)** to measure the strength and direction of the monotonic relationship between these variables.\n",
    "\n",
    "### **a. Ranks for \\( X \\) and \\( Y \\)**\n",
    "- **Ranks for \\( X \\)**:\n",
    "  [ 5.0, 10.5, 8.5, 12.0, 15.0, 4.0, 13.5, 2.0, 17.0, 7.0, 8.5, 13.5, 1.0, 19.0, 10.5, 20.0, 6.0, 3.0, 18.0, 16.0 ]\n",
    "- **Ranks for \\( Y \\)**:\n",
    "  [ 5.0, 10.0, 8.0, 11.0, 15.0, 4.0, 13.0, 2.0, 18.0, 7.0, 9.0, 14.0, 1.0, 19.0, 12.0, 20.0, 6.0, 3.0, 17.0, 16.0 ]\n",
    "\n",
    "### **b. Differences ( \\( d \\) ) Between Ranks and Squared Differences ( \\( d^2 \\) )**\n",
    "- **Differences ( \\( d \\) ):**\n",
    "  [ 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, -1.0, 0.0, -0.5, -0.5, 0.0, 0.0, -1.5, 0.0, 0.0, 0.0, 1.0, 0.0 ]\n",
    "- **Squared Differences ( \\( d^2 \\) ):**\n",
    "  [ 0.00, 0.25, 0.25, 1.00, 0.00, 0.00, 0.25, 0.00, 1.00, 0.00, 0.25, 0.25, 0.00, 0.00, 2.25, 0.00, 0.00, 0.00, 1.00, 0.00 ]\n",
    "\n",
    "### **c. Sum of Squared Differences ( \\( \\sum d^2 \\) )**\n",
    "- **Sum of \\( d^2 \\):** **6.5**\n",
    "\n",
    "### **d. Spearman Rank Correlation Coefficient ( \\( \\rho \\) )**\n",
    "- **Spearman Rank Correlation Coefficient ( \\( \\rho \\) ) = 0.995**\n",
    "- **Interpretation:**\n",
    "  - Since **\\( \\rho = 0.995 \\)** is **very close to 1**, this indicates an **extremely strong positive monotonic relationship**.\n",
    "  - This means that **as study hours increase, exam scores consistently increase in a predictable pattern**.\n"
   ],
   "id": "5b1d6ef906d24d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:40:34.330775Z",
     "start_time": "2025-02-28T16:40:32.584808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import rankdata, spearmanr\n",
    "\n",
    "# Compute ranks for X and Y\n",
    "ranks_X = rankdata(X)\n",
    "ranks_Y = rankdata(Y)\n",
    "\n",
    "# Compute differences (d) and squared differences (d^2)\n",
    "d = ranks_X - ranks_Y\n",
    "d_squared = d ** 2\n",
    "\n",
    "# Sum of squared differences (Î£ d^2)\n",
    "sum_d_squared = np.sum(d_squared)\n",
    "\n",
    "# Calculate Spearman's Rank Correlation Coefficient (Ï)\n",
    "spearman_rho, _ = spearmanr(X, Y)\n",
    "\n",
    "# Interpretation\n",
    "spearman_strength = \"strong\" if abs(spearman_rho) > 0.7 else \"moderate\" if abs(spearman_rho) > 0.4 else \"weak\"\n",
    "spearman_direction = \"positive\" if spearman_rho > 0 else \"negative\"\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Spearman Rank Correlation Analysis':^70}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Ranks for X: {ranks_X}\")\n",
    "print(f\"Ranks for Y: {ranks_Y}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Differences (d) between ranks: {d}\")\n",
    "print(f\"Squared Differences (dÂ²): {d_squared}\")\n",
    "print(f\"Sum of Squared Differences (Î£ dÂ²): {sum_d_squared}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Spearman Rank Correlation Coefficient (Ï): {spearman_rho:.3f}\")\n",
    "print(f\"Correlation Strength: {spearman_strength.capitalize()}\")\n",
    "print(f\"Correlation Direction: {spearman_direction.capitalize()}\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "b9358a4cc3d8406e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                  Spearman Rank Correlation Analysis                  \n",
      "======================================================================\n",
      "Ranks for X: [ 5.  10.5  8.5 12.  15.   4.  13.5  2.  17.   7.   8.5 13.5  1.  19.\n",
      " 10.5 20.   6.   3.  18.  16. ]\n",
      "Ranks for Y: [ 5. 10.  8. 11. 15.  4. 13.  2. 18.  7.  9. 14.  1. 19. 12. 20.  6.  3.\n",
      " 17. 16.]\n",
      "----------------------------------------------------------------------\n",
      "Differences (d) between ranks: [ 0.   0.5  0.5  1.   0.   0.   0.5  0.  -1.   0.  -0.5 -0.5  0.   0.\n",
      " -1.5  0.   0.   0.   1.   0. ]\n",
      "Squared Differences (dÂ²): [0.   0.25 0.25 1.   0.   0.   0.25 0.   1.   0.   0.25 0.25 0.   0.\n",
      " 2.25 0.   0.   0.   1.   0.  ]\n",
      "Sum of Squared Differences (Î£ dÂ²): 6.5\n",
      "----------------------------------------------------------------------\n",
      "Spearman Rank Correlation Coefficient (Ï): 0.995\n",
      "Correlation Strength: Strong\n",
      "Correlation Direction: Positive\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 8:**\n",
    "\n",
    "### **Key Findings from Question 6 and Question 7**\n",
    "- **Pearson Correlation Coefficient (\\( r \\))**: **0.994**\n",
    "- **Spearman Rank Correlation Coefficient (\\( \\rho \\))**: **0.995**\n",
    "- Both values are **very close to 1**, indicating a **strong positive correlation** in both measures.\n",
    "\n",
    "### **Similarities Between Pearsonâ€™s and Spearmanâ€™s Correlation**\n",
    "1. **Strong Positive Relationship**\n",
    "   - Both **Pearsonâ€™s (\\( r \\)) and Spearmanâ€™s (\\( \\rho \\)) values are near 1**, confirming that as study hours increase, exam scores also increase.\n",
    "   - This suggests a **strong association** between the variables in both linear and monotonic terms.\n",
    "\n",
    "2. **Direction of Relationship**\n",
    "   - Both coefficients are **positive**, meaning the relationship between study hours and exam scores is **positively correlated**â€”students who study more tend to score higher.\n",
    "\n",
    "### **Differences Between Pearsonâ€™s and Spearmanâ€™s Correlation**\n",
    "1. **Type of Relationship Measured**\n",
    "   - **Pearsonâ€™s Correlation (\\( r \\))** measures the **strength of a linear relationship**.\n",
    "   - **Spearmanâ€™s Correlation (\\( \\rho \\))** measures the **strength of a monotonic relationship**, which applies even if the relationship is non-linear.\n",
    "\n",
    "2. **Handling of Data Distribution and Outliers**\n",
    "   - **Pearsonâ€™s correlation is sensitive to outliers**, meaning extreme values can impact the correlation coefficient.\n",
    "   - **Spearmanâ€™s correlation is rank-based**, meaning it is less affected by outliers and measures how well the data maintains a consistent increasing or decreasing trend.\n",
    "\n",
    "3. **Applicability to Different Data Types**\n",
    "   - **Pearsonâ€™s correlation** requires **normally distributed, continuous data** and assumes a **linear relationship**.\n",
    "   - **Spearmanâ€™s correlation** can be used for **ordinal, non-linear, or non-normally distributed data** because it ranks values instead of using exact numbers.\n",
    "\n",
    "### **Conclusion**\n",
    "- In this dataset, **both Pearsonâ€™s and Spearmanâ€™s correlation values are extremely high**, meaning **study hours and exam scores have a very strong positive relationship**.\n",
    "- The **small difference between \\( r \\) and \\( \\rho \\)** suggests that the relationship between study hours and exam scores is **both linear and monotonic**, meaning students who study more tend to perform better in a predictable, proportional manner.\n",
    "- If the data had **outliers or a non-linear relationship**, **Spearmanâ€™s \\( \\rho \\)** would be the preferred measure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Interpretation**\n",
    "Since **\\( r = 0.994 \\) and \\(rho = 0.995 \\)**, we can confidently conclude that the relationship between **study hours and exam scores is highly correlated in both linear and rank-based measures**. There is no significant difference between the two correlation coefficients in this dataset, but **Spearmanâ€™s correlation would be more reliable in cases with non-linear trends or outliers**.\n"
   ],
   "id": "caf59f6051090a71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 9: Comparison of Means and Standard Deviations**\n",
    "\n",
    "In each pair of distributions, we compare which set has a **greater mean (ğœ‡)** and **greater standard deviation (ğœ)** without explicitly calculating them.\n",
    "\n",
    "---\n",
    "\n",
    "### **(a)**\n",
    "#### **Distributions:**\n",
    "- **i.** 3, 5, 5, 5, 8, 11, 11, 11, 13\n",
    "- **ii.** 3, 5, 5, 5, 8, 11, 11, 11, 20\n",
    "\n",
    "#### **Comparison:**\n",
    "- **Greater Mean:** **ii**\n",
    "  - The values in **ii** are identical to **i**, except the last value is **20 instead of 13**.\n",
    "  - Since the largest number is **greater**, the **mean increases**.\n",
    "- **Greater Standard Deviation:** **ii**\n",
    "  - The spread of values is **wider** in **ii** due to the larger extreme value (**20 vs. 13**), increasing **ğœ**.\n",
    "\n",
    "---\n",
    "\n",
    "### **(b)**\n",
    "#### **Distributions:**\n",
    "- **i.** -20, 0, 0, 0, 15, 25, 30, 30\n",
    "- **ii.** -40, 0, 0, 0, 15, 25, 30, 30\n",
    "\n",
    "#### **Comparison:**\n",
    "- **Greater Mean:** **i**\n",
    "  - Both distributions are identical except that **ii** has **-40 instead of -20**.\n",
    "  - Since **-40 is smaller than -20**, the **mean of ii is lower**, making **i have the greater mean**.\n",
    "- **Greater Standard Deviation:** **ii**\n",
    "  - The **larger negative value (-40 vs. -20)** in **ii** creates a **greater spread**, increasing **ğœ**.\n",
    "\n",
    "---\n",
    "\n",
    "### **(c)**\n",
    "#### **Distributions:**\n",
    "- **i.** 0, 2, 4, 6, 8, 10\n",
    "- **ii.** 20, 22, 24, 26, 28, 30\n",
    "\n",
    "#### **Comparison:**\n",
    "- **Greater Mean:** **ii**\n",
    "  - The values in **ii** are simply **20 units larger** than those in **i**, shifting the **entire distribution upward**, resulting in a **higher mean**.\n",
    "- **Greater Standard Deviation:** **Neither; they are the same**\n",
    "  - Both distributions have **identical spacing between numbers**.\n",
    "  - **Shifting** all numbers **by a constant amount (20)** does **not change** the standard deviation.\n",
    "\n",
    "---\n",
    "\n",
    "### **(d)**\n",
    "#### **Distributions:**\n",
    "- **i.** 100, 200, 300, 400, 500\n",
    "- **ii.** 0, 50, 300, 550, 600\n",
    "\n",
    "#### **Comparison:**\n",
    "- **Greater Mean:** **Neither; both have the same mean**\n",
    "  - The sum of values in both distributions is the same, so their **means are equal**.\n",
    "- **Greater Standard Deviation:** **ii**\n",
    "  - The values in **ii** are more **spread out** (especially **0 and 600** vs. **100 and 500** in i).\n",
    "  - **A wider spread increases standard deviation (ğœ).**\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Summary Table**\n",
    "| **Pair** | **Greater Mean (ğœ‡)** | **Greater Standard Deviation (ğœ)** |\n",
    "|---------|---------------------|-------------------------------|\n",
    "| **(a)** | ii | ii |\n",
    "| **(b)** | i  | ii |\n",
    "| **(c)** | ii | Same |\n",
    "| **(d)** | Same | ii |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- **Mean (ğœ‡) is affected by shifts in values**, while **standard deviation (ğœ) is affected by spread.**\n",
    "- Adding a **larger extreme value** increases both mean and standard deviation.\n",
    "- **Shifting all values by a constant does not change standard deviation.**\n"
   ],
   "id": "8ccae045aa2116c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:40:34.344768Z",
     "start_time": "2025-02-28T16:40:34.339331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the distributions\n",
    "distributions = {\n",
    "    \"(a) i\": np.array([3, 5, 5, 5, 8, 11, 11, 11, 13]),\n",
    "    \"(a) ii\": np.array([3, 5, 5, 5, 8, 11, 11, 11, 20]),\n",
    "    \"(b) i\": np.array([-20, 0, 0, 0, 15, 25, 30, 30]),\n",
    "    \"(b) ii\": np.array([-40, 0, 0, 0, 15, 25, 30, 30]),\n",
    "    \"(c) i\": np.array([0, 2, 4, 6, 8, 10]),\n",
    "    \"(c) ii\": np.array([20, 22, 24, 26, 28, 30]),\n",
    "    \"(d) i\": np.array([100, 200, 300, 400, 500]),\n",
    "    \"(d) ii\": np.array([0, 50, 300, 550, 600])\n",
    "}\n",
    "\n",
    "# Compute mean and standard deviation for each distribution\n",
    "results = {}\n",
    "for label, data in distributions.items():\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data, ddof=1)  # Sample standard deviation\n",
    "    results[label] = {\"Mean\": mean, \"Std Dev\": std_dev}\n",
    "\n",
    "# Compare means and standard deviations for each pair\n",
    "comparison_results = []\n",
    "pairs = [(\"(a) i\", \"(a) ii\"), (\"(b) i\", \"(b) ii\"), (\"(c) i\", \"(c) ii\"), (\"(d) i\", \"(d) ii\")]\n",
    "\n",
    "for pair in pairs:\n",
    "    mean_winner = pair[0] if results[pair[0]][\"Mean\"] > results[pair[1]][\"Mean\"] else pair[1] if results[pair[0]][\"Mean\"] < results[pair[1]][\"Mean\"] else \"Same\"\n",
    "    std_dev_winner = pair[0] if results[pair[0]][\"Std Dev\"] > results[pair[1]][\"Std Dev\"] else pair[1] if results[pair[0]][\"Std Dev\"] < results[pair[1]][\"Std Dev\"] else \"Same\"\n",
    "    comparison_results.append((pair[0], pair[1], mean_winner, std_dev_winner))\n",
    "\n",
    "# Print results in a readable format\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Comparison of Means and Standard Deviations':^50}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pair in comparison_results:\n",
    "    print(f\"Pair: {pair[0]} vs {pair[1]}\")\n",
    "    print(f\"  - Greater Mean: {pair[2]}\")\n",
    "    print(f\"  - Greater Standard Deviation: {pair[3]}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "5f3960606541dcca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   Comparison of Means and Standard Deviations    \n",
      "==================================================\n",
      "Pair: (a) i vs (a) ii\n",
      "  - Greater Mean: (a) ii\n",
      "  - Greater Standard Deviation: (a) ii\n",
      "--------------------------------------------------\n",
      "Pair: (b) i vs (b) ii\n",
      "  - Greater Mean: (b) i\n",
      "  - Greater Standard Deviation: (b) ii\n",
      "--------------------------------------------------\n",
      "Pair: (c) i vs (c) ii\n",
      "  - Greater Mean: (c) ii\n",
      "  - Greater Standard Deviation: Same\n",
      "--------------------------------------------------\n",
      "Pair: (d) i vs (d) ii\n",
      "  - Greater Mean: Same\n",
      "  - Greater Standard Deviation: (d) ii\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 10:**\n",
    "\n",
    "We are given the probabilities:\n",
    "- P(A) = 0.3\n",
    "- P(B) = 0.7\n",
    "\n",
    "### **(a) Can we compute P(A and B) with only P(A) and P(B)?**\n",
    "No, we **cannot** determine P(A and B) unless we know whether **A and B are independent or dependent**. If the events are **dependent**, we would need additional information, such as P(A|B) or P(B|A), to compute P(A and B).\n",
    "\n",
    "---\n",
    "\n",
    "### **(b) Assuming A and B are independent:**\n",
    "For **independent** events, we use the following formulas:\n",
    "\n",
    "1. **P(A and B)**\n",
    "   P(A and B) = P(A) Ã— P(B)\n",
    "   P(A and B) = 0.3 Ã— 0.7 = 0.21\n",
    "\n",
    "2. **P(A or B)**\n",
    "   P(A or B) = P(A) + P(B) - P(A and B)\n",
    "   P(A or B) = 0.3 + 0.7 - 0.21 = 0.79\n",
    "\n",
    "3. **P(A|B) (Conditional Probability)**\n",
    "   P(A|B) = P(A and B)/P(B)\n",
    "   P(A|B) = 0.21/0.7 = 0.3"
   ],
   "id": "a112f013893eb1fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:40:34.363523Z",
     "start_time": "2025-02-28T16:40:34.361184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given probabilities\n",
    "P_A = 0.3  # Probability of event A\n",
    "P_B = 0.7  # Probability of event B\n",
    "\n",
    "# (a) P(A and B) cannot be determined without knowing dependence\n",
    "\n",
    "# (b) Assuming A and B are independent:\n",
    "P_A_and_B = P_A * P_B  # P(A and B) = P(A) * P(B) for independent events\n",
    "P_A_or_B = P_A + P_B - P_A_and_B  # P(A or B) = P(A) + P(B) - P(A and B)\n",
    "P_A_given_B = P_A_and_B / P_B  # P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "# Output results\n",
    "print(f\"P(A and B) = {P_A_and_B:.4f}\")\n",
    "print(f\"P(A or B) = {P_A_or_B:.4f}\")\n",
    "print(f\"P(A | B) = {P_A_given_B:.4f}\")"
   ],
   "id": "28db988c58bdb049",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A and B) = 0.2100\n",
      "P(A or B) = 0.7900\n",
      "P(A | B) = 0.3000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Question 11:**\n",
    "\n",
    "Probability and statistics are closely related but serve different purposes. **Probability** is the mathematical framework that deals with predicting the likelihood of different outcomes before any data is collected. Itâ€™s all about working from a known model to possible results. For example, if you flip a fair coin, probability tells you thereâ€™s a **50%** chance of landing on heads and a **50%** chance of tails. Itâ€™s theoretical, dealing with outcomes that **havenâ€™t happened yet**.\n",
    "\n",
    "**Statistics**, on the other hand, works **backward**â€”it starts with real-world data and tries to make sense of it. Instead of assuming a known model, statistics **analyzes** data to determine patterns, relationships, and trends. For example, if you flip a coin **100 times** and get heads **55 times**, statistics helps determine if the coin is fair or biased based on the data collected. It's all about making inferences **after observing real events**.\n",
    "\n",
    "The key difference is that **probability is predictive**, and **statistics is analytical**. Probability goes from **assumptions to outcomes**, while statistics goes from **outcomes to conclusions**. Probability helps us set expectations, while statistics helps us understand and verify reality."
   ],
   "id": "5ad384f88953af3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 12: Disease Screening Test Analysis\n",
    "\n",
    "A disease affects a small percentage of the population, and a screening test is used to detect it. However, like all tests, it is not perfectly accurateâ€”false positives and false negatives occur. The following table shows the test outcomes:\n",
    "\n",
    "| Outcome              | Disease Present (D+) | Disease Absent (D-) | Total |\n",
    "|----------------------|----------------------|---------------------|-------|\n",
    "| Test Positive (T+)   | 95                   | 105                 | 200   |\n",
    "| Test Negative (T-)   | 5                    | 1795                | 1800  |\n",
    "| **Total**            | **100**              | **1900**            | **2000** |\n",
    "\n",
    "We calculate the following probabilities:\n",
    "\n",
    "### (b) P(D+) - The Prior Probability of Having the Disease\n",
    "P(D+) = Total with disease / Total population = 100/2000 = 0.05 (5%)\n",
    "\n",
    "### (c) P(T+) - The Total Probability of Testing Positive\n",
    "P(T+) = Total positive tests / Total population = 200/2000 = 0.10 (10%)\n",
    "\n",
    "### (d) P(D+ | T+) - The Probability of Having the Disease Given a Positive Test\n",
    "P(D+ | T+) = True Positives / Total positive tests = 95/200 = 0.475 (47.5%)\n",
    "\n",
    "*Interpretation: If someone tests positive, there is only a 47.5% chance that they actually have the disease.*\n",
    "\n",
    "### (e) P(T+ | D+) - The Probability of Testing Positive Given the Presence of the Disease (Sensitivity)\n",
    "P(T+ | D+) = True Positives / Total with disease = 95/100 = 0.95 (95%)\n",
    "\n",
    "*Interpretation: The test correctly identifies 95% of individuals who have the disease.*\n",
    "\n",
    "### (f) P(T- | D-) - The Probability of Testing Negative Given the Absence of the Disease (Specificity)\n",
    "P(T- | D-) = True Negatives / Total healthy individuals = 1795/1900 â‰ˆ 0.9447 (94.47%)\n",
    "\n",
    "*Interpretation: The test correctly identifies approximately 94.47% of healthy individuals as disease-free.*\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of the Test Effectiveness\n",
    "\n",
    "- **Sensitivity (95%)** indicates the test is very effective at detecting those with the disease.\n",
    "- **Specificity (94.47%)** shows that the test is also effective at identifying healthy individuals.\n",
    "- However, the **positive predictive value** P(D+ | T+) is only 47.5%, meaning that more than half of the individuals who test positive might not actually have the disease. This highlights the importance of confirmatory testing before making clinical decisions based solely on this screening test."
   ],
   "id": "36bda408269ba3b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T17:47:48.334101Z",
     "start_time": "2025-02-28T17:47:48.331220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given data\n",
    "total_population = 2000\n",
    "disease_present = 100\n",
    "disease_absent = 1900\n",
    "test_positive = 200\n",
    "test_negative = 1800\n",
    "true_positives = 95\n",
    "false_positives = 105\n",
    "true_negatives = 1795\n",
    "false_negatives = 5\n",
    "\n",
    "# (a) P(D+) - Prior probability of having the disease\n",
    "P_D_plus = disease_present / total_population\n",
    "\n",
    "# (b) P(T+) - Total probability of testing positive\n",
    "P_T_plus = test_positive / total_population\n",
    "\n",
    "# (c) P(D+ | T+) - Probability of having the disease given a positive test result\n",
    "P_D_plus_given_T_plus = true_positives / test_positive\n",
    "\n",
    "# (d) P(T+ | D+) - Sensitivity (True Positive Rate)\n",
    "P_T_plus_given_D_plus = true_positives / disease_present\n",
    "\n",
    "# (e) P(T- | D-) - Specificity (True Negative Rate)\n",
    "P_T_minus_given_D_minus = true_negatives / disease_absent\n",
    "\n",
    "# Print results\n",
    "print(f\"P(D+) = {P_D_plus:.4f} (5%)\")\n",
    "print(f\"P(T+) = {P_T_plus:.4f} (10%)\")\n",
    "print(f\"P(D+ | T+) = {P_D_plus_given_T_plus:.4f} (47.5%)\")\n",
    "print(f\"P(T+ | D+) = {P_T_plus_given_D_plus:.4f} (95%)\")\n",
    "print(f\"P(T- | D-) = {P_T_minus_given_D_minus:.4f} (94.47%)\")\n"
   ],
   "id": "edaf6fd95e21e388",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(D+) = 0.0500 (5%)\n",
      "P(T+) = 0.1000 (10%)\n",
      "P(D+ | T+) = 0.4750 (47.5%)\n",
      "P(T+ | D+) = 0.9500 (95%)\n",
      "P(T- | D-) = 0.9447 (94.47%)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Used Resources**\n",
    "\n",
    "AI-generated responses were used for **clarifying formulas, coding structure, and statistical concepts**. Answers were rewritten and verified using additional sources.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Generative AI Assistance (ChatGPT, February 2025)**\n",
    "I used ChatGPT for **coding assistance, mathematical explanations, and Jupyter-related commands**. All responses were rewritten, manually tested, and verified using additional resources.\n",
    "\n",
    "| **Question/Topic**                                    | **Prompt Used**                                                                           | **How I Used It**                                                                                                                      |\n",
    "|:------------------------------------------------------|:------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Q2: Publicly Available Datasets**                   | *\"What are some unique publicly available datasets for analysis?\"*                        | AI helped identify **niche datasets** (UFC fight stats, Chess data, Exoplanet data), but I manually selected and verified the sources. |\n",
    "| **Q3: Experimental Design (Coke vs. Diet Coke Test)** | *\"How do you properly design a blind taste test for two sodas?\"*                          | Used AI for **basic structure**, then rewrote with references from experimental design sources.                                        |\n",
    "| **Q4: Logarithm Use Cases**                           | *\"How are logarithms used in real-world data science applications?\"*                      | AI suggested **three broad use cases**, which I refined and confirmed with **Wolfram MathWorld**.                                      |\n",
    "| **Q5: Correlation vs. Causation**                     | *\"Explain correlation vs. causation with examples.\"*                                      | AI provided general **examples**, but I rewrote them and verified with **Google searches**.                                            |\n",
    "| **Q6-Q7: Correlation Calculations**                   | *\"How do I manually calculate Pearson and Spearman correlation?\"*                         | Used AI for **formula breakdown**, but I manually worked through examples before coding.                                               |\n",
    "| **Q10: Probability Computations**                     | *\"Write Python code to calculate conditional probability P(A                              | B) given P(A) and P(B).\"*                                                                                                              | AI helped with **code structure**, but I checked the logic with **google** and manual calculations. |\n",
    "| **Q12: Sensitivity & Specificity Computations**       | *\"How do you calculate sensitivity, specificity, and Bayes' theorem for a medical test?\"* | AI provided formulas, but I **verified all calculations** using Google and statistical resources.                                      |\n",
    "| **Jupyter Notebook: Export Issues**                   | *\"How do I export a Jupyter Notebook to PDF in PyCharm?\"*                                 | AI suggested `nbconvert`, but I tested multiple methods including **HTML to PDF conversion**.                                          |\n",
    "| **PyCharm: Running Jupyter Notebooks**                | *\"How do I set up Jupyter Notebooks in PyCharm and run a `.ipynb` file?\"*                 | AI provided steps, but I also confirmed by setting up PyCharmâ€™s **Jupyter support manually**.                                          |\n",
    "| **Python Script Conversion**                          | *\"How do I convert a Jupyter Notebook to a `.py` script?\"*                                | AI suggested `jupyter nbconvert --to script`, which I tested and used for exporting my `.py` file.                                     |\n",
    "\n",
    "---\n"
   ],
   "id": "f3bb55991f9e2864"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "51718214a2cbbcd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
